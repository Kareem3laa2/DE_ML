{"timestamp":"2025-06-30T21:12:51.186857","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-06-30T21:12:51.187319","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/telco_to_hdfs_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-06-30T21:12:51.204070","level":"info","event":"Tmp dir root location: /tmp","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:51.204856","level":"info","event":"Running command: ['/usr/bin/bash', '-c', 'docker exec spark-master /spark/bin/spark-submit /opt/spark-apps/get_clean.py']","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:51.214213","level":"info","event":"Output:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:52.519936","level":"info","event":"25/06/30 21:12:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:53.019480","level":"info","event":"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:53.027164","level":"info","event":"25/06/30 21:12:53 INFO SparkContext: Running Spark version 3.1.1","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:53.064266","level":"info","event":"25/06/30 21:12:53 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:53.064469","level":"info","event":"25/06/30 21:12:53 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:53.064541","level":"info","event":"25/06/30 21:12:53 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:53.065616","level":"info","event":"25/06/30 21:12:53 INFO SparkContext: Submitted application: TelcoExploration","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:53.088113","level":"info","event":"25/06/30 21:12:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:53.102342","level":"info","event":"25/06/30 21:12:53 INFO ResourceProfile: Limiting resource is cpu","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:53.102529","level":"info","event":"25/06/30 21:12:53 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:53.143746","level":"info","event":"25/06/30 21:12:53 INFO SecurityManager: Changing view acls to: root","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:53.143990","level":"info","event":"25/06/30 21:12:53 INFO SecurityManager: Changing modify acls to: root","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:53.144110","level":"info","event":"25/06/30 21:12:53 INFO SecurityManager: Changing view acls groups to:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:53.144184","level":"info","event":"25/06/30 21:12:53 INFO SecurityManager: Changing modify acls groups to:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:53.144252","level":"info","event":"25/06/30 21:12:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:53.321674","level":"info","event":"25/06/30 21:12:53 INFO Utils: Successfully started service 'sparkDriver' on port 44327.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:53.350177","level":"info","event":"25/06/30 21:12:53 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:53.373217","level":"info","event":"25/06/30 21:12:53 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:53.390703","level":"info","event":"25/06/30 21:12:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:53.391090","level":"info","event":"25/06/30 21:12:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:53.394638","level":"info","event":"25/06/30 21:12:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:53.408581","level":"info","event":"25/06/30 21:12:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cccb4d8a-aeb2-4367-92e4-052bb9ad92a6","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:53.429397","level":"info","event":"25/06/30 21:12:53 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:53.443878","level":"info","event":"25/06/30 21:12:53 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:53.628481","level":"info","event":"25/06/30 21:12:53 INFO Utils: Successfully started service 'SparkUI' on port 4040.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:53.675996","level":"info","event":"25/06/30 21:12:53 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://76fff13ec683:4040","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:53.865431","level":"info","event":"25/06/30 21:12:53 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:53.924355","level":"info","event":"25/06/30 21:12:53 INFO TransportClientFactory: Successfully created connection to spark-master/172.21.0.4:7077 after 33 ms (0 ms spent in bootstraps)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:54.015466","level":"info","event":"25/06/30 21:12:54 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250630211254-0004","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:54.017356","level":"info","event":"25/06/30 21:12:54 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250630211254-0004/0 on worker-20250630194602-172.21.0.6-38987 (172.21.0.6:38987) with 12 core(s)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:54.018774","level":"info","event":"25/06/30 21:12:54 INFO StandaloneSchedulerBackend: Granted executor ID app-20250630211254-0004/0 on hostPort 172.21.0.6:38987 with 12 core(s), 1024.0 MiB RAM","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:54.022197","level":"info","event":"25/06/30 21:12:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34937.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:54.022377","level":"info","event":"25/06/30 21:12:54 INFO NettyBlockTransferService: Server created on 76fff13ec683:34937","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:54.023647","level":"info","event":"25/06/30 21:12:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:54.031720","level":"info","event":"25/06/30 21:12:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 76fff13ec683, 34937, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:54.035263","level":"info","event":"25/06/30 21:12:54 INFO BlockManagerMasterEndpoint: Registering block manager 76fff13ec683:34937 with 366.3 MiB RAM, BlockManagerId(driver, 76fff13ec683, 34937, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:54.038287","level":"info","event":"25/06/30 21:12:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 76fff13ec683, 34937, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:54.039545","level":"info","event":"25/06/30 21:12:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 76fff13ec683, 34937, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:54.053093","level":"info","event":"25/06/30 21:12:54 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250630211254-0004/0 is now RUNNING","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:54.207784","level":"info","event":"25/06/30 21:12:54 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:54.449236","level":"info","event":"25/06/30 21:12:54 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/spark-warehouse').","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:54.449619","level":"info","event":"25/06/30 21:12:54 INFO SharedState: Warehouse path is 'file:/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:55.618971","level":"info","event":"25/06/30 21:12:55 INFO InMemoryFileIndex: It took 54 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:55.683629","level":"info","event":"25/06/30 21:12:55 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:55.946303","level":"info","event":"25/06/30 21:12:55 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.21.0.6:49016) with ID 0,  ResourceProfileId 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:56.064583","level":"info","event":"25/06/30 21:12:56 INFO BlockManagerMasterEndpoint: Registering block manager 172.21.0.6:44843 with 366.3 MiB RAM, BlockManagerId(0, 172.21.0.6, 44843, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:57.348940","level":"info","event":"25/06/30 21:12:57 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:57.352792","level":"info","event":"25/06/30 21:12:57 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:57.356074","level":"info","event":"25/06/30 21:12:57 INFO FileSourceStrategy: Output Data Schema: struct<value: string>","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:57.975734","level":"info","event":"25/06/30 21:12:57 INFO CodeGenerator: Code generated in 233.213045 ms","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:58.020961","level":"info","event":"25/06/30 21:12:58 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.4 KiB, free 366.0 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:58.060925","level":"info","event":"25/06/30 21:12:58 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:58.063726","level":"info","event":"25/06/30 21:12:58 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 76fff13ec683:34937 (size: 27.5 KiB, free: 366.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:58.066048","level":"info","event":"25/06/30 21:12:58 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:58.072385","level":"info","event":"25/06/30 21:12:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:58.163677","level":"info","event":"25/06/30 21:12:58 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:58.175765","level":"info","event":"25/06/30 21:12:58 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:58.175904","level":"info","event":"25/06/30 21:12:58 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:58.175965","level":"info","event":"25/06/30 21:12:58 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:58.177037","level":"info","event":"25/06/30 21:12:58 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:58.180762","level":"info","event":"25/06/30 21:12:58 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:58.230094","level":"info","event":"25/06/30 21:12:58 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.8 KiB, free 366.0 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:58.232059","level":"info","event":"25/06/30 21:12:58 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 366.0 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:58.232749","level":"info","event":"25/06/30 21:12:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 76fff13ec683:34937 (size: 5.4 KiB, free: 366.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:58.233435","level":"info","event":"25/06/30 21:12:58 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:58.244002","level":"info","event":"25/06/30 21:12:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:58.244830","level":"info","event":"25/06/30 21:12:58 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:58.274834","level":"info","event":"25/06/30 21:12:58 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.21.0.6, executor 0, partition 0, ANY, 4876 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:58.489625","level":"info","event":"25/06/30 21:12:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.21.0.6:44843 (size: 5.4 KiB, free: 366.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:59.294456","level":"info","event":"25/06/30 21:12:59 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.21.0.6:44843 (size: 27.5 KiB, free: 366.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:59.963838","level":"info","event":"25/06/30 21:12:59 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1696 ms on 172.21.0.6 (executor 0) (1/1)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:59.965728","level":"info","event":"25/06/30 21:12:59 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:59.971108","level":"info","event":"25/06/30 21:12:59 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 1.776 s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:59.974412","level":"info","event":"25/06/30 21:12:59 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:59.974589","level":"info","event":"25/06/30 21:12:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:59.976956","level":"info","event":"25/06/30 21:12:59 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 1.813077 s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:12:59.995752","level":"info","event":"25/06/30 21:12:59 INFO CodeGenerator: Code generated in 10.074221 ms","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:00.041682","level":"info","event":"25/06/30 21:13:00 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:00.041808","level":"info","event":"25/06/30 21:13:00 INFO FileSourceStrategy: Post-Scan Filters:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:00.041849","level":"info","event":"25/06/30 21:13:00 INFO FileSourceStrategy: Output Data Schema: struct<value: string>","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:00.049992","level":"info","event":"25/06/30 21:13:00 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.4 KiB, free 365.7 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:00.066161","level":"info","event":"25/06/30 21:13:00 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:00.066720","level":"info","event":"25/06/30 21:13:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 76fff13ec683:34937 (size: 27.5 KiB, free: 366.2 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:00.067289","level":"info","event":"25/06/30 21:13:00 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:00.068368","level":"info","event":"25/06/30 21:13:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:00.125162","level":"info","event":"25/06/30 21:13:00 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:00.126515","level":"info","event":"25/06/30 21:13:00 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:00.126829","level":"info","event":"25/06/30 21:13:00 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:00.126991","level":"info","event":"25/06/30 21:13:00 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:00.127972","level":"info","event":"25/06/30 21:13:00 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:00.131279","level":"info","event":"25/06/30 21:13:00 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:00.156053","level":"info","event":"25/06/30 21:13:00 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.8 KiB, free 365.6 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:00.158869","level":"info","event":"25/06/30 21:13:00 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 365.6 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:00.159911","level":"info","event":"25/06/30 21:13:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 76fff13ec683:34937 (size: 8.0 KiB, free: 366.2 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:00.160937","level":"info","event":"25/06/30 21:13:00 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1383","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:00.162372","level":"info","event":"25/06/30 21:13:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:00.162594","level":"info","event":"25/06/30 21:13:00 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:00.164740","level":"info","event":"25/06/30 21:13:00 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.21.0.6, executor 0, partition 0, ANY, 4876 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:00.191209","level":"info","event":"25/06/30 21:13:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.21.0.6:44843 (size: 8.0 KiB, free: 366.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:01.233718","level":"info","event":"25/06/30 21:13:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.21.0.6:44843 (size: 27.5 KiB, free: 366.2 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:01.448306","level":"info","event":"25/06/30 21:13:01 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1285 ms on 172.21.0.6 (executor 0) (1/1)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:01.448634","level":"info","event":"25/06/30 21:13:01 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:01.449307","level":"info","event":"25/06/30 21:13:01 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 1.316 s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:01.449423","level":"info","event":"25/06/30 21:13:01 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:01.449480","level":"info","event":"25/06/30 21:13:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:01.450913","level":"info","event":"25/06/30 21:13:01 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 1.325573 s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.004883","level":"info","event":"25/06/30 21:13:02 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.005052","level":"info","event":"25/06/30 21:13:02 INFO FileSourceStrategy: Post-Scan Filters:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.005315","level":"info","event":"25/06/30 21:13:02 INFO FileSourceStrategy: Output Data Schema: struct<customerID: string, gender: string, SeniorCitizen: int, Partner: string, Dependents: string ... 19 more fields>","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.030861","level":"info","event":"25/06/30 21:13:02 WARN NetUtils: Unable to wrap exception of type class org.apache.hadoop.ipc.RpcException: it has no (String) constructor","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.031050","level":"info","event":"java.lang.NoSuchMethodException: org.apache.hadoop.ipc.RpcException.<init>(java.lang.String)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.031127","level":"info","event":"\tat java.lang.Class.getConstructor0(Class.java:3082)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.031182","level":"info","event":"\tat java.lang.Class.getConstructor(Class.java:1825)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.031232","level":"info","event":"\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:830)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.031280","level":"info","event":"\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:806)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.031329","level":"info","event":"\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.031372","level":"info","event":"\tat org.apache.hadoop.ipc.Client.call(Client.java:1457)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.031413","level":"info","event":"\tat org.apache.hadoop.ipc.Client.call(Client.java:1367)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.031455","level":"info","event":"\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.031493","level":"info","event":"\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.031533","level":"info","event":"\tat com.sun.proxy.$Proxy19.getFileInfo(Unknown Source)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.031574","level":"info","event":"\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:903)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.031614","level":"info","event":"\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.031657","level":"info","event":"\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.031699","level":"info","event":"\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.031743","level":"info","event":"\tat java.lang.reflect.Method.invoke(Method.java:498)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.031787","level":"info","event":"\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.031830","level":"info","event":"\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.031875","level":"info","event":"\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.031921","level":"info","event":"\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.031965","level":"info","event":"\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.032014","level":"info","event":"\tat com.sun.proxy.$Proxy20.getFileInfo(Unknown Source)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.032063","level":"info","event":"\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1665)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.032108","level":"info","event":"\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1582)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.032152","level":"info","event":"\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1579)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.032198","level":"info","event":"\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.032242","level":"info","event":"\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1594)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.032287","level":"info","event":"\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1683)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.032331","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:119)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.032376","level":"info","event":"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.032452","level":"info","event":"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.032503","level":"info","event":"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.032549","level":"info","event":"\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.032593","level":"info","event":"\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.032640","level":"info","event":"\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.032771","level":"info","event":"\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.032836","level":"info","event":"\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.032932","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.033027","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.033086","level":"info","event":"\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.033165","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.033218","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.033266","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.033310","level":"info","event":"\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.033351","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.033395","level":"info","event":"\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.033494","level":"info","event":"\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.033547","level":"info","event":"\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.033590","level":"info","event":"\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.033630","level":"info","event":"\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.033670","level":"info","event":"\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.033711","level":"info","event":"\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.033756","level":"info","event":"\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.033800","level":"info","event":"\tat java.lang.reflect.Method.invoke(Method.java:498)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.033842","level":"info","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.033884","level":"info","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.033926","level":"info","event":"\tat py4j.Gateway.invoke(Gateway.java:282)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.033972","level":"info","event":"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.034012","level":"info","event":"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.034051","level":"info","event":"\tat py4j.GatewayConnection.run(GatewayConnection.java:238)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.034151","level":"info","event":"\tat java.lang.Thread.run(Thread.java:748)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.043434","level":"info","event":"Traceback (most recent call last):","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.043594","level":"info","event":"  File \"/opt/spark-apps/get_clean.py\", line 23, in <module>","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.045080","level":"info","event":"    df_cleaned.write.mode(\"overwrite\").parquet(\"hdfs://namenode:9870/user/telco/cleaned/telco_cleaned.parquet\")","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.045195","level":"info","event":"  File \"/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 1249, in parquet","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.045247","level":"info","event":"  File \"/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1305, in __call__","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.045522","level":"info","event":"  File \"/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 111, in deco","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.045793","level":"info","event":"  File \"/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 328, in get_return_value","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.046977","level":"info","event":"py4j.protocol.Py4JJavaError: An error occurred while calling o65.parquet.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.047100","level":"info","event":": java.io.IOException: Failed on local exception: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length; Host Details : local host is: \"76fff13ec683/172.21.0.4\"; destination host is: \"namenode\":9870;","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.047163","level":"info","event":"\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:816)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.047215","level":"info","event":"\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.047252","level":"info","event":"\tat org.apache.hadoop.ipc.Client.call(Client.java:1457)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.047331","level":"info","event":"\tat org.apache.hadoop.ipc.Client.call(Client.java:1367)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.047396","level":"info","event":"\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.047430","level":"info","event":"\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.047476","level":"info","event":"\tat com.sun.proxy.$Proxy19.getFileInfo(Unknown Source)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.047509","level":"info","event":"\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:903)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.047535","level":"info","event":"\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.047560","level":"info","event":"\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.047583","level":"info","event":"\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.047623","level":"info","event":"\tat java.lang.reflect.Method.invoke(Method.java:498)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.047663","level":"info","event":"\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.047691","level":"info","event":"\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.047715","level":"info","event":"\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.047740","level":"info","event":"\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.047773","level":"info","event":"\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.047809","level":"info","event":"\tat com.sun.proxy.$Proxy20.getFileInfo(Unknown Source)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.047835","level":"info","event":"\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1665)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.047860","level":"info","event":"\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1582)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.047883","level":"info","event":"\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1579)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.047905","level":"info","event":"\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.047928","level":"info","event":"\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1594)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.047974","level":"info","event":"\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1683)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048002","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:119)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048027","level":"info","event":"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048051","level":"info","event":"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048074","level":"info","event":"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048119","level":"info","event":"\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048152","level":"info","event":"\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048192","level":"info","event":"\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048217","level":"info","event":"\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048241","level":"info","event":"\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048264","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048303","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048342","level":"info","event":"\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048371","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048395","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048428","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048491","level":"info","event":"\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048527","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048569","level":"info","event":"\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048599","level":"info","event":"\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048634","level":"info","event":"\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048695","level":"info","event":"\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048723","level":"info","event":"\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048747","level":"info","event":"\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048775","level":"info","event":"\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048798","level":"info","event":"\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048821","level":"info","event":"\tat java.lang.reflect.Method.invoke(Method.java:498)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048866","level":"info","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048897","level":"info","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048925","level":"info","event":"\tat py4j.Gateway.invoke(Gateway.java:282)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048950","level":"info","event":"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048973","level":"info","event":"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.048996","level":"info","event":"\tat py4j.GatewayConnection.run(GatewayConnection.java:238)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.049028","level":"info","event":"\tat java.lang.Thread.run(Thread.java:748)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.049067","level":"info","event":"Caused by: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.049094","level":"info","event":"\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1830)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.049119","level":"info","event":"\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1173)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.049142","level":"info","event":"\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1069)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.049166","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.065277","level":"info","event":"25/06/30 21:13:02 INFO SparkContext: Invoking stop() from shutdown hook","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.072948","level":"info","event":"25/06/30 21:13:02 INFO SparkUI: Stopped Spark web UI at http://76fff13ec683:4040","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.075099","level":"info","event":"25/06/30 21:13:02 INFO StandaloneSchedulerBackend: Shutting down all executors","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.075234","level":"info","event":"25/06/30 21:13:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.091346","level":"info","event":"25/06/30 21:13:02 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.102811","level":"info","event":"25/06/30 21:13:02 INFO MemoryStore: MemoryStore cleared","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.103193","level":"info","event":"25/06/30 21:13:02 INFO BlockManager: BlockManager stopped","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.110328","level":"info","event":"25/06/30 21:13:02 INFO BlockManagerMaster: BlockManagerMaster stopped","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.112442","level":"info","event":"25/06/30 21:13:02 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.119222","level":"info","event":"25/06/30 21:13:02 INFO SparkContext: Successfully stopped SparkContext","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.119353","level":"info","event":"25/06/30 21:13:02 INFO ShutdownHookManager: Shutdown hook called","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.119741","level":"info","event":"25/06/30 21:13:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-073ddcd5-2645-4124-9395-75b0de3a46e7","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.123555","level":"info","event":"25/06/30 21:13:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-51261be8-ad88-466b-b666-5d568006889c","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.126778","level":"info","event":"25/06/30 21:13:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-51261be8-ad88-466b-b666-5d568006889c/pyspark-28d0177f-a733-449d-bb91-5cec91aebb7d","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.284427","level":"info","event":"Command exited with return code 1","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:13:02.285443","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"AirflowException","exc_value":"Bash command failed. The command returned a non-zero exit code 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":825,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1088,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":408,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/bash.py","lineno":233,"name":"execute"}]}]}
