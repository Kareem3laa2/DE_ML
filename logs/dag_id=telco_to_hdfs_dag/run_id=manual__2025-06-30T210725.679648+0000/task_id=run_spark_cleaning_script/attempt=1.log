{"timestamp":"2025-06-30T21:07:30.326169","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-06-30T21:07:30.326632","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/telco_to_hdfs_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-06-30T21:07:30.342845","level":"info","event":"Tmp dir root location: /tmp","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:30.344137","level":"info","event":"Running command: ['/usr/bin/bash', '-c', 'docker exec spark-master /spark/bin/spark-submit /opt/spark-apps/get_clean.py']","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:30.352090","level":"info","event":"Output:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:31.887725","level":"info","event":"25/06/30 21:07:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:32.427439","level":"info","event":"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:32.435574","level":"info","event":"25/06/30 21:07:32 INFO SparkContext: Running Spark version 3.1.1","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:32.470182","level":"info","event":"25/06/30 21:07:32 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:32.470374","level":"info","event":"25/06/30 21:07:32 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:32.470442","level":"info","event":"25/06/30 21:07:32 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:32.470496","level":"info","event":"25/06/30 21:07:32 INFO SparkContext: Submitted application: TelcoExploration","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:32.492751","level":"info","event":"25/06/30 21:07:32 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:32.506370","level":"info","event":"25/06/30 21:07:32 INFO ResourceProfile: Limiting resource is cpu","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:32.506775","level":"info","event":"25/06/30 21:07:32 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:32.553662","level":"info","event":"25/06/30 21:07:32 INFO SecurityManager: Changing view acls to: root","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:32.553776","level":"info","event":"25/06/30 21:07:32 INFO SecurityManager: Changing modify acls to: root","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:32.553825","level":"info","event":"25/06/30 21:07:32 INFO SecurityManager: Changing view acls groups to:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:32.553863","level":"info","event":"25/06/30 21:07:32 INFO SecurityManager: Changing modify acls groups to:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:32.553905","level":"info","event":"25/06/30 21:07:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:32.754574","level":"info","event":"25/06/30 21:07:32 INFO Utils: Successfully started service 'sparkDriver' on port 45687.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:32.782420","level":"info","event":"25/06/30 21:07:32 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:32.813233","level":"info","event":"25/06/30 21:07:32 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:32.836469","level":"info","event":"25/06/30 21:07:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:32.836753","level":"info","event":"25/06/30 21:07:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:32.841052","level":"info","event":"25/06/30 21:07:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:32.857586","level":"info","event":"25/06/30 21:07:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3c901dee-5049-40b1-927b-ea091d2d4f80","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:32.879698","level":"info","event":"25/06/30 21:07:32 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:32.895300","level":"info","event":"25/06/30 21:07:32 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:33.116773","level":"info","event":"25/06/30 21:07:33 INFO Utils: Successfully started service 'SparkUI' on port 4040.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:33.168056","level":"info","event":"25/06/30 21:07:33 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://76fff13ec683:4040","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:33.377833","level":"info","event":"25/06/30 21:07:33 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:33.421860","level":"info","event":"25/06/30 21:07:33 INFO TransportClientFactory: Successfully created connection to spark-master/172.21.0.4:7077 after 26 ms (0 ms spent in bootstraps)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:33.508794","level":"info","event":"25/06/30 21:07:33 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250630210733-0002","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:33.511373","level":"info","event":"25/06/30 21:07:33 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250630210733-0002/0 on worker-20250630194602-172.21.0.6-38987 (172.21.0.6:38987) with 12 core(s)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:33.512785","level":"info","event":"25/06/30 21:07:33 INFO StandaloneSchedulerBackend: Granted executor ID app-20250630210733-0002/0 on hostPort 172.21.0.6:38987 with 12 core(s), 1024.0 MiB RAM","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:33.516286","level":"info","event":"25/06/30 21:07:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44791.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:33.516494","level":"info","event":"25/06/30 21:07:33 INFO NettyBlockTransferService: Server created on 76fff13ec683:44791","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:33.518534","level":"info","event":"25/06/30 21:07:33 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:33.526850","level":"info","event":"25/06/30 21:07:33 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 76fff13ec683, 44791, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:33.529784","level":"info","event":"25/06/30 21:07:33 INFO BlockManagerMasterEndpoint: Registering block manager 76fff13ec683:44791 with 366.3 MiB RAM, BlockManagerId(driver, 76fff13ec683, 44791, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:33.532383","level":"info","event":"25/06/30 21:07:33 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 76fff13ec683, 44791, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:33.533827","level":"info","event":"25/06/30 21:07:33 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 76fff13ec683, 44791, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:33.598102","level":"info","event":"25/06/30 21:07:33 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250630210733-0002/0 is now RUNNING","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:33.741504","level":"info","event":"25/06/30 21:07:33 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:34.066553","level":"info","event":"25/06/30 21:07:34 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/spark-warehouse').","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:34.066753","level":"info","event":"25/06/30 21:07:34 INFO SharedState: Warehouse path is 'file:/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:35.303680","level":"info","event":"25/06/30 21:07:35 INFO InMemoryFileIndex: It took 63 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:35.363261","level":"info","event":"25/06/30 21:07:35 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:35.597935","level":"info","event":"25/06/30 21:07:35 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.21.0.6:59276) with ID 0,  ResourceProfileId 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:35.718128","level":"info","event":"25/06/30 21:07:35 INFO BlockManagerMasterEndpoint: Registering block manager 172.21.0.6:34901 with 366.3 MiB RAM, BlockManagerId(0, 172.21.0.6, 34901, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:37.232324","level":"info","event":"25/06/30 21:07:37 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:37.236032","level":"info","event":"25/06/30 21:07:37 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:37.239636","level":"info","event":"25/06/30 21:07:37 INFO FileSourceStrategy: Output Data Schema: struct<value: string>","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:37.935348","level":"info","event":"25/06/30 21:07:37 INFO CodeGenerator: Code generated in 273.081723 ms","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:37.986338","level":"info","event":"25/06/30 21:07:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.4 KiB, free 366.0 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:38.034771","level":"info","event":"25/06/30 21:07:38 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:38.038884","level":"info","event":"25/06/30 21:07:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 76fff13ec683:44791 (size: 27.5 KiB, free: 366.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:38.042295","level":"info","event":"25/06/30 21:07:38 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:38.050664","level":"info","event":"25/06/30 21:07:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:38.149729","level":"info","event":"25/06/30 21:07:38 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:38.163050","level":"info","event":"25/06/30 21:07:38 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:38.163212","level":"info","event":"25/06/30 21:07:38 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:38.163329","level":"info","event":"25/06/30 21:07:38 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:38.164583","level":"info","event":"25/06/30 21:07:38 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:38.168750","level":"info","event":"25/06/30 21:07:38 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:38.222971","level":"info","event":"25/06/30 21:07:38 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.8 KiB, free 366.0 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:38.227265","level":"info","event":"25/06/30 21:07:38 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 366.0 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:38.228305","level":"info","event":"25/06/30 21:07:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 76fff13ec683:44791 (size: 5.4 KiB, free: 366.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:38.229799","level":"info","event":"25/06/30 21:07:38 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:38.244094","level":"info","event":"25/06/30 21:07:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:38.244468","level":"info","event":"25/06/30 21:07:38 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:38.277432","level":"info","event":"25/06/30 21:07:38 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.21.0.6, executor 0, partition 0, ANY, 4876 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:38.524305","level":"info","event":"25/06/30 21:07:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.21.0.6:34901 (size: 5.4 KiB, free: 366.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:39.366346","level":"info","event":"25/06/30 21:07:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.21.0.6:34901 (size: 27.5 KiB, free: 366.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.085308","level":"info","event":"25/06/30 21:07:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1817 ms on 172.21.0.6 (executor 0) (1/1)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.087426","level":"info","event":"25/06/30 21:07:40 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.094410","level":"info","event":"25/06/30 21:07:40 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 1.910 s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.097739","level":"info","event":"25/06/30 21:07:40 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.097929","level":"info","event":"25/06/30 21:07:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.100308","level":"info","event":"25/06/30 21:07:40 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 1.950289 s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.122861","level":"info","event":"25/06/30 21:07:40 INFO CodeGenerator: Code generated in 11.310698 ms","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.169992","level":"info","event":"25/06/30 21:07:40 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.170134","level":"info","event":"25/06/30 21:07:40 INFO FileSourceStrategy: Post-Scan Filters:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.170195","level":"info","event":"25/06/30 21:07:40 INFO FileSourceStrategy: Output Data Schema: struct<value: string>","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.177374","level":"info","event":"25/06/30 21:07:40 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.4 KiB, free 365.7 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.202278","level":"info","event":"25/06/30 21:07:40 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.203172","level":"info","event":"25/06/30 21:07:40 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 76fff13ec683:44791 (size: 27.5 KiB, free: 366.2 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.203804","level":"info","event":"25/06/30 21:07:40 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.204558","level":"info","event":"25/06/30 21:07:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.259683","level":"info","event":"25/06/30 21:07:40 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.261170","level":"info","event":"25/06/30 21:07:40 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.261370","level":"info","event":"25/06/30 21:07:40 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.261455","level":"info","event":"25/06/30 21:07:40 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.261508","level":"info","event":"25/06/30 21:07:40 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.262416","level":"info","event":"25/06/30 21:07:40 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.286300","level":"info","event":"25/06/30 21:07:40 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.8 KiB, free 365.6 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.288073","level":"info","event":"25/06/30 21:07:40 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 365.6 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.288627","level":"info","event":"25/06/30 21:07:40 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 76fff13ec683:44791 (size: 8.0 KiB, free: 366.2 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.289090","level":"info","event":"25/06/30 21:07:40 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1383","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.289799","level":"info","event":"25/06/30 21:07:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.289906","level":"info","event":"25/06/30 21:07:40 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.291406","level":"info","event":"25/06/30 21:07:40 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.21.0.6, executor 0, partition 0, ANY, 4876 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:40.317145","level":"info","event":"25/06/30 21:07:40 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.21.0.6:34901 (size: 8.0 KiB, free: 366.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:41.520174","level":"info","event":"25/06/30 21:07:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.21.0.6:34901 (size: 27.5 KiB, free: 366.2 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:41.771996","level":"info","event":"25/06/30 21:07:41 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1480 ms on 172.21.0.6 (executor 0) (1/1)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:41.772186","level":"info","event":"25/06/30 21:07:41 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:41.773237","level":"info","event":"25/06/30 21:07:41 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 1.509 s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:41.773563","level":"info","event":"25/06/30 21:07:41 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:41.773662","level":"info","event":"25/06/30 21:07:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:41.774560","level":"info","event":"25/06/30 21:07:41 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 1.514701 s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.318150","level":"info","event":"25/06/30 21:07:42 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.318295","level":"info","event":"25/06/30 21:07:42 INFO FileSourceStrategy: Post-Scan Filters:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.318947","level":"info","event":"25/06/30 21:07:42 INFO FileSourceStrategy: Output Data Schema: struct<customerID: string, gender: string, SeniorCitizen: int, Partner: string, Dependents: string ... 19 more fields>","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.358112","level":"info","event":"25/06/30 21:07:42 WARN NetUtils: Unable to wrap exception of type class org.apache.hadoop.ipc.RpcException: it has no (String) constructor","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.358264","level":"info","event":"java.lang.NoSuchMethodException: org.apache.hadoop.ipc.RpcException.<init>(java.lang.String)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.358324","level":"info","event":"\tat java.lang.Class.getConstructor0(Class.java:3082)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.358369","level":"info","event":"\tat java.lang.Class.getConstructor(Class.java:1825)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.358397","level":"info","event":"\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:830)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.358424","level":"info","event":"\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:806)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.358449","level":"info","event":"\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.358483","level":"info","event":"\tat org.apache.hadoop.ipc.Client.call(Client.java:1457)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.358517","level":"info","event":"\tat org.apache.hadoop.ipc.Client.call(Client.java:1367)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.358551","level":"info","event":"\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.358583","level":"info","event":"\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.358619","level":"info","event":"\tat com.sun.proxy.$Proxy19.getFileInfo(Unknown Source)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.358654","level":"info","event":"\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:903)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.358691","level":"info","event":"\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.358728","level":"info","event":"\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.358766","level":"info","event":"\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.358807","level":"info","event":"\tat java.lang.reflect.Method.invoke(Method.java:498)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.358847","level":"info","event":"\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.358890","level":"info","event":"\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.358930","level":"info","event":"\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.358969","level":"info","event":"\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.359009","level":"info","event":"\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.359048","level":"info","event":"\tat com.sun.proxy.$Proxy20.getFileInfo(Unknown Source)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.359086","level":"info","event":"\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1665)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.359125","level":"info","event":"\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1582)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.359161","level":"info","event":"\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1579)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.359197","level":"info","event":"\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.359234","level":"info","event":"\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1594)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.359269","level":"info","event":"\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1683)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.359304","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:119)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.359339","level":"info","event":"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.359372","level":"info","event":"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.359409","level":"info","event":"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.359446","level":"info","event":"\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.359487","level":"info","event":"\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.359528","level":"info","event":"\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.359566","level":"info","event":"\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.359602","level":"info","event":"\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.359636","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.359698","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.359758","level":"info","event":"\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.359798","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.359918","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.359993","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.360029","level":"info","event":"\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.360086","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.360127","level":"info","event":"\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.360198","level":"info","event":"\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.360271","level":"info","event":"\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.360313","level":"info","event":"\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.360341","level":"info","event":"\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.360365","level":"info","event":"\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.360389","level":"info","event":"\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.360441","level":"info","event":"\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.360474","level":"info","event":"\tat java.lang.reflect.Method.invoke(Method.java:498)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.360500","level":"info","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.360525","level":"info","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.360548","level":"info","event":"\tat py4j.Gateway.invoke(Gateway.java:282)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.360574","level":"info","event":"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.360597","level":"info","event":"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.360650","level":"info","event":"\tat py4j.GatewayConnection.run(GatewayConnection.java:238)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.360688","level":"info","event":"\tat java.lang.Thread.run(Thread.java:748)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.372147","level":"info","event":"Traceback (most recent call last):","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.372261","level":"info","event":"  File \"/opt/spark-apps/get_clean.py\", line 23, in <module>","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.373773","level":"info","event":"    df_cleaned.write.mode(\"overwrite\").parquet(\"hdfs://namenode:9870/user/telco/cleaned/telco_cleaned.parquet\")","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.373921","level":"info","event":"  File \"/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 1249, in parquet","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.374297","level":"info","event":"  File \"/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1305, in __call__","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.374590","level":"info","event":"  File \"/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 111, in deco","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.374872","level":"info","event":"  File \"/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 328, in get_return_value","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.376380","level":"info","event":"py4j.protocol.Py4JJavaError: An error occurred while calling o65.parquet.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.376534","level":"info","event":": java.io.IOException: Failed on local exception: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length; Host Details : local host is: \"76fff13ec683/172.21.0.4\"; destination host is: \"namenode\":9870;","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.376610","level":"info","event":"\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:816)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.376662","level":"info","event":"\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.376713","level":"info","event":"\tat org.apache.hadoop.ipc.Client.call(Client.java:1457)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.376761","level":"info","event":"\tat org.apache.hadoop.ipc.Client.call(Client.java:1367)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.376809","level":"info","event":"\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.376856","level":"info","event":"\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.376902","level":"info","event":"\tat com.sun.proxy.$Proxy19.getFileInfo(Unknown Source)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.376950","level":"info","event":"\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:903)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.376997","level":"info","event":"\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.377045","level":"info","event":"\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.377091","level":"info","event":"\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.377138","level":"info","event":"\tat java.lang.reflect.Method.invoke(Method.java:498)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.377186","level":"info","event":"\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.377235","level":"info","event":"\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.377283","level":"info","event":"\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.377330","level":"info","event":"\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.377409","level":"info","event":"\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.377465","level":"info","event":"\tat com.sun.proxy.$Proxy20.getFileInfo(Unknown Source)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.377516","level":"info","event":"\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1665)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.377565","level":"info","event":"\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1582)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.377613","level":"info","event":"\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1579)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.377661","level":"info","event":"\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.377708","level":"info","event":"\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1594)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.377757","level":"info","event":"\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1683)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.377806","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:119)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.377854","level":"info","event":"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.377899","level":"info","event":"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.377934","level":"info","event":"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.377975","level":"info","event":"\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.378042","level":"info","event":"\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.378095","level":"info","event":"\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.378141","level":"info","event":"\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.378190","level":"info","event":"\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.378235","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.378278","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.378322","level":"info","event":"\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.378362","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.378399","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.378436","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.378472","level":"info","event":"\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.378509","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.378543","level":"info","event":"\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.378579","level":"info","event":"\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.378615","level":"info","event":"\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.378666","level":"info","event":"\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.378704","level":"info","event":"\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.378740","level":"info","event":"\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.378775","level":"info","event":"\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.378808","level":"info","event":"\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.378933","level":"info","event":"\tat java.lang.reflect.Method.invoke(Method.java:498)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.378982","level":"info","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.379018","level":"info","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.379053","level":"info","event":"\tat py4j.Gateway.invoke(Gateway.java:282)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.379134","level":"info","event":"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.379213","level":"info","event":"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.379292","level":"info","event":"\tat py4j.GatewayConnection.run(GatewayConnection.java:238)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.379370","level":"info","event":"\tat java.lang.Thread.run(Thread.java:748)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.379417","level":"info","event":"Caused by: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.379456","level":"info","event":"\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1830)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.379494","level":"info","event":"\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1173)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.379529","level":"info","event":"\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1069)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.379564","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.398185","level":"info","event":"25/06/30 21:07:42 INFO SparkContext: Invoking stop() from shutdown hook","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.407717","level":"info","event":"25/06/30 21:07:42 INFO SparkUI: Stopped Spark web UI at http://76fff13ec683:4040","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.410681","level":"info","event":"25/06/30 21:07:42 INFO StandaloneSchedulerBackend: Shutting down all executors","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.410861","level":"info","event":"25/06/30 21:07:42 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.431488","level":"info","event":"25/06/30 21:07:42 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.448755","level":"info","event":"25/06/30 21:07:42 INFO MemoryStore: MemoryStore cleared","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.448901","level":"info","event":"25/06/30 21:07:42 INFO BlockManager: BlockManager stopped","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.460269","level":"info","event":"25/06/30 21:07:42 INFO BlockManagerMaster: BlockManagerMaster stopped","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.464769","level":"info","event":"25/06/30 21:07:42 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.472623","level":"info","event":"25/06/30 21:07:42 INFO SparkContext: Successfully stopped SparkContext","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.472818","level":"info","event":"25/06/30 21:07:42 INFO ShutdownHookManager: Shutdown hook called","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.473346","level":"info","event":"25/06/30 21:07:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-6a528d39-0296-43db-bb12-f049c7cfcad8","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.477674","level":"info","event":"25/06/30 21:07:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-8375ef57-240c-4c55-974b-30dc404a4b5c","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.481020","level":"info","event":"25/06/30 21:07:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-8375ef57-240c-4c55-974b-30dc404a4b5c/pyspark-601eb795-822c-459c-b99c-5626d997a463","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.630942","level":"info","event":"Command exited with return code 1","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-30T21:07:42.631791","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"AirflowException","exc_value":"Bash command failed. The command returned a non-zero exit code 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":825,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1088,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":408,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/bash.py","lineno":233,"name":"execute"}]}]}
